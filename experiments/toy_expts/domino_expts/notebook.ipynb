{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "IPython.notebook.set_autosave_interval(10000)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 10 seconds\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2  \n",
    "%autosave 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import random, os, sys, argparse\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0,'/xxx/home/xxx/xxxp/xxx/projects/shortcut_detection_and_mitigation/experiments/toy_expts/')\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user hyperparams\n",
    "model = 'resnet18' # resnet18, vgg16, densenet121\n",
    "train_csv = '/xxx/home/xxx/xxxp/xxx/projects/shortcut_detection_and_mitigation/experiments/toy_expts/domino_expts/top_svhn_bot_fmnist_2class_ro_1p0.csv'\n",
    "test_csv = '/xxx/home/xxx/xxxp/xxx/projects/shortcut_detection_and_mitigation/experiments/toy_expts/domino_expts/top_blank_bot_fmnist_2class_ro_1p0.csv'\n",
    "expt_name = f'temp_{int(random.random()*100000)}'# f'resnet18_top_mnist_bot_kmnist_{int(random.random()*100000)}'\n",
    "save_dir = '/xxx/home/xxx/xxxp/xxx/projects/shortcut_detection_and_mitigation/experiments/toy_expts/domino_expts/output/'\n",
    "\n",
    "num_epochs = 50\n",
    "lr = 0.1\n",
    "seed = 10\n",
    "num_ch = 3 # num of channels in image\n",
    "num_embs = 2000 # 1500 for mnist, 10k for cifar10\n",
    "K = 29 # K neighbours\n",
    "num_test_imgs = 20 # num of test images for plotting PD\n",
    "lp_norm = 1 # for computing KNN\n",
    "knn_pos_thresh = 0.5\n",
    "knn_neg_thresh = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the seed\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(train_csv)\n",
    "df_test = pd.read_csv(test_csv).sample(500)\n",
    "# df_test = df_test[df_test['test_split']==0].sample(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, class_field, transform, csv_path=None, df=None):\n",
    "        self.class_field = class_field        \n",
    "        self.transform = transform\n",
    "        if df is not None:\n",
    "            self.df = df\n",
    "        else:\n",
    "            self.csvpath = csvpath\n",
    "            self.df = pd.read_csv(self.csvpath)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = str(self.df.iloc[idx][\"path\"])\n",
    "        lab = int(self.df.iloc[idx][self.class_field])\n",
    "        img = Image.open(img_path)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)  \n",
    "\n",
    "        return {\"img\":img, \"lab\":lab, \"idx\":idx, \"file_name\" : img_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Resize((64,32)),\n",
    "])\n",
    "trainset = ToyDataset(class_field=['bottom'], transform=trans, df=df_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=256, shuffle=True, num_workers=2)\n",
    "testset = ToyDataset(class_field=['bottom'], transform=trans, df=df_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=256, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building model..\n",
      "\n",
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [00:08<00:00,  4.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 38 Loss: 4.022 | Acc: 70.885% (6805/9600)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:03<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 Loss: 0.532 | Acc: 89.400% (447/500)\n",
      "Saving..\n",
      "\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [00:08<00:00,  4.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 38 Loss: 0.228 | Acc: 93.625% (8988/9600)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  3.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 Loss: 0.430 | Acc: 89.200% (446/500)\n",
      "Saving..\n",
      "\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [00:08<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 38 Loss: 0.123 | Acc: 95.938% (9210/9600)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 Loss: 0.240 | Acc: 94.000% (470/500)\n",
      "Saving..\n",
      "\n",
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [00:08<00:00,  4.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 38 Loss: 0.092 | Acc: 96.979% (9310/9600)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  3.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 Loss: 0.427 | Acc: 85.400% (427/500)\n",
      "Saving..\n",
      "\n",
      "Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [00:08<00:00,  4.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 38 Loss: 0.079 | Acc: 97.438% (9354/9600)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 Loss: 0.218 | Acc: 93.400% (467/500)\n",
      "Saving..\n",
      "\n",
      "Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [00:08<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 38 Loss: 0.069 | Acc: 97.781% (9387/9600)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 Loss: 0.124 | Acc: 95.800% (479/500)\n",
      "Saving..\n",
      "\n",
      "Epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 5/38 [00:11<01:17,  2.36s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4858/3902856022.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4858/3902856022.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "best_acc = 0  # best test accuracy\n",
    "\n",
    "\n",
    "# Model\n",
    "print('==> Building model..')\n",
    "if model=='resnet18':\n",
    "    net = ResNet18(num_channels=num_ch)\n",
    "elif model=='vgg16':\n",
    "    net = VGG('VGG16',num_channels=num_ch)\n",
    "elif model=='densenet121':\n",
    "    net = DenseNet121()\n",
    "net.linear = nn.Linear(in_features=1024,out_features=10,bias=True)\n",
    "net = net.to(device)\n",
    "    \n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr,\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "\n",
    "# Training\n",
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, batch in enumerate(tqdm(trainloader)):\n",
    "        inputs = batch['img']\n",
    "        targets = batch['lab']\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    print(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                 % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(tqdm(testloader)):\n",
    "            inputs = batch['img']\n",
    "            targets = batch['lab']\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    print(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                 % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "#     if acc > best_acc:\n",
    "#         print('Saving..')\n",
    "#         state = {\n",
    "#             'net': net.state_dict(),\n",
    "#             'acc': acc,\n",
    "#             'epoch': epoch,\n",
    "#         }\n",
    "# #         if not os.path.isdir(f'{args['expt_name']}_checkpoint'):\n",
    "# #             os.mkdir('checkpoint')\n",
    "#         torch.save(state, os.path.join(save_dir,f'{expt_name}_ep{epoch}.pt'))\n",
    "#         best_acc = acc\n",
    "    print('Saving..')\n",
    "    state = {\n",
    "        'net': net.state_dict(),\n",
    "        'acc': acc,\n",
    "        'epoch': epoch,\n",
    "    }\n",
    "    torch.save(state, os.path.join(save_dir,f'{expt_name}.pt'))\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/jet/home/nmurali/asc170022p/nmurali/projects/shortcut_detection_and_mitigation/experiments/toy_expts/domino_expts/top_svhn_bot_fmnist_2class_ro_1p0.csv'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop the whole code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain Train Subset Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load checkpoint\n",
    "# expt_name = 'resnet18_top_cifar10_bot_fmnist_64x32_60744_ep6'\n",
    "if model=='resnet18':\n",
    "    net = ResNet18(num_channels=num_ch)\n",
    "    net.linear = nn.Linear(in_features=1024,out_features=10,bias=True)\n",
    "    net = nn.DataParallel(net)\n",
    "elif model=='vgg16':\n",
    "    net = nn.DataParallel(VGG('VGG16',num_channels=num_ch))\n",
    "net.load_state_dict(torch.load(os.path.join(save_dir,f'{expt_name}.pt'))['net'])\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding hook function for resnet18\n",
    "# def add_resnet18_hooks(net, hook):\n",
    "#     net.module.bn1.register_forward_hook(hook)\n",
    "    \n",
    "#     net.module.layer1[0].bn1.register_forward_hook(hook)\n",
    "#     net.module.layer1[0].shortcut.register_forward_hook(hook)\n",
    "    \n",
    "#     net.module.layer1[1].bn1.register_forward_hook(hook)\n",
    "#     net.module.layer1[1].shortcut.register_forward_hook(hook)\n",
    "\n",
    "#     net.module.layer2[0].bn1.register_forward_hook(hook)\n",
    "#     net.module.layer2[0].shortcut.register_forward_hook(hook)\n",
    "    \n",
    "#     net.module.layer2[1].bn1.register_forward_hook(hook)\n",
    "#     net.module.layer2[1].shortcut.register_forward_hook(hook)\n",
    "    \n",
    "#     net.module.layer3[0].bn1.register_forward_hook(hook)\n",
    "#     net.module.layer3[0].shortcut.register_forward_hook(hook)\n",
    "    \n",
    "#     net.module.layer3[1].bn1.register_forward_hook(hook)\n",
    "#     net.module.layer3[1].shortcut.register_forward_hook(hook)\n",
    "    \n",
    "#     net.module.layer4[0].bn1.register_forward_hook(hook)\n",
    "#     net.module.layer4[0].shortcut.register_forward_hook(hook)\n",
    "    \n",
    "#     net.module.layer4[1].bn1.register_forward_hook(hook)\n",
    "#     net.module.layer4[1].shortcut.register_forward_hook(hook)\n",
    "        \n",
    "#     return net\n",
    "\n",
    "def add_resnet18_hooks(net, hook):\n",
    "    net.module.bn1.register_forward_hook(hook)\n",
    "    \n",
    "    net.module.layer1[0].conv1.register_forward_hook(hook)\n",
    "    net.module.layer1[0].conv2.register_forward_hook(hook)\n",
    "    \n",
    "    net.module.layer1[1].conv1.register_forward_hook(hook)\n",
    "    net.module.layer1[1].conv2.register_forward_hook(hook)\n",
    "\n",
    "    net.module.layer2[0].conv1.register_forward_hook(hook)\n",
    "    net.module.layer2[0].conv2.register_forward_hook(hook)\n",
    "    \n",
    "    net.module.layer2[1].conv1.register_forward_hook(hook)\n",
    "    net.module.layer2[1].conv2.register_forward_hook(hook)\n",
    "    \n",
    "    net.module.layer3[0].conv1.register_forward_hook(hook)\n",
    "    net.module.layer3[0].conv2.register_forward_hook(hook)\n",
    "    \n",
    "    net.module.layer3[1].conv1.register_forward_hook(hook)\n",
    "    net.module.layer3[1].conv2.register_forward_hook(hook)\n",
    "    \n",
    "    net.module.layer4[0].conv1.register_forward_hook(hook)\n",
    "    net.module.layer4[0].conv2.register_forward_hook(hook)\n",
    "    \n",
    "    net.module.layer4[1].conv1.register_forward_hook(hook)\n",
    "    net.module.layer4[1].conv2.register_forward_hook(hook)\n",
    "        \n",
    "    return net\n",
    "\n",
    "def add_vgg16_hooks(net, hook):\n",
    "    net.module.features[0].register_forward_hook(hook)\n",
    "    net.module.features[3].register_forward_hook(hook)\n",
    "    net.module.features[7].register_forward_hook(hook)\n",
    "    net.module.features[10].register_forward_hook(hook)\n",
    "    net.module.features[14].register_forward_hook(hook)\n",
    "    net.module.features[17].register_forward_hook(hook)\n",
    "    net.module.features[20].register_forward_hook(hook)\n",
    "    net.module.features[24].register_forward_hook(hook)\n",
    "    net.module.features[27].register_forward_hook(hook)\n",
    "    net.module.features[30].register_forward_hook(hook)\n",
    "    net.module.features[34].register_forward_hook(hook)\n",
    "    net.module.features[37].register_forward_hook(hook)\n",
    "    net.module.features[40].register_forward_hook(hook)        \n",
    "    return net\n",
    "\n",
    "def add_densenet121_hooks(net, hook):\n",
    "    \n",
    "    for idx,layer in enumerate(net.module.dense1):\n",
    "        if idx%2==0:\n",
    "            layer.register_forward_hook(hook)\n",
    "        \n",
    "    for idx,layer in enumerate(net.module.dense2):\n",
    "        if idx%2==0:\n",
    "            layer.register_forward_hook(hook)\n",
    "        \n",
    "    for idx,layer in enumerate(net.module.dense3):\n",
    "        if idx%2==0:\n",
    "            layer.register_forward_hook(hook)\n",
    "        \n",
    "    for idx,layer in enumerate(net.module.dense4):\n",
    "        if idx%2==0:\n",
    "            layer.register_forward_hook(hook)\n",
    "        \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_maps = []  # This will be a list of Tensors, each representing a feature map\n",
    "def hook_feat_map(mod, inp, out):\n",
    "#     out = torch.nn.functional.interpolate(out,(24,12))\n",
    "#     feature_maps.append(torch.mean(out,dim=[2,3]))\n",
    "    feature_maps.append(torch.reshape(out, (out.shape[0],-1)))\n",
    "\n",
    "if model=='resnet18':\n",
    "    net = add_resnet18_hooks(net, hook_feat_map)\n",
    "elif model=='vgg16':\n",
    "    net = add_vgg16_hooks(net, hook_feat_map)\n",
    "elif model=='densenet121':\n",
    "    net = add_densenet121_hooks(net, hook_feat_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_cpu(arr):\n",
    "    for idx,x in enumerate(arr):\n",
    "        arr[idx] = x.to('cpu')\n",
    "    return arr\n",
    "\n",
    "def print_memory_profile(s):\n",
    "    # print GPU memory\n",
    "    t = torch.cuda.get_device_properties(0).total_memory\n",
    "    r = torch.cuda.memory_reserved(0)\n",
    "    a = torch.cuda.memory_allocated(0)\n",
    "    print(s)\n",
    "    print(t/1024**3,r/1024**3,a/1024**3)\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "perm = torch.randperm(len(trainset))\n",
    "inds = perm[:num_embs]\n",
    "\n",
    "labs = []\n",
    "samples = torch.empty((0,3,64,32))\n",
    "for i in tqdm(inds):\n",
    "    i = int(i)\n",
    "    labs.append(trainset[i]['lab'])\n",
    "    samples = torch.cat((samples,trainset[i]['img'].unsqueeze(0)))\n",
    "labs = torch.tensor(labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_resized = samples\n",
    "train_subset = torch.utils.data.TensorDataset(samples_resized,labs)\n",
    "trainloader2 = torch.utils.data.DataLoader(train_subset, batch_size=128, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for saving pkl file of layer embeddings\n",
    "save_path = os.path.join(save_dir,f'{expt_name}.pkl')\n",
    "trainloader2 = torch.utils.data.DataLoader(train_subset, batch_size=30000, shuffle=True, num_workers=2)\n",
    "\n",
    "handle = open(save_path, \"wb\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "    for b_idx,batch in enumerate(tqdm(trainloader2)):        \n",
    "        # print GPU memory\n",
    "        print_memory_profile('Initial')\n",
    "        \n",
    "#         if dataset=='mnist'  or dataset=='kmnist' or dataset=='fmnist':\n",
    "#             imgs = batch[0].unsqueeze(1).to('cuda')\n",
    "#         elif dataset=='svhn':\n",
    "#             imgs = batch[0].permute(0,3,1,2).to('cuda')\n",
    "#         else:\n",
    "#             imgs = batch[0].permute(0,3,1,2).to('cuda')\n",
    "        imgs = batch[0].to('cuda')\n",
    "        labels = batch[1]\n",
    "        \n",
    "        feature_maps = []\n",
    "        out = net(imgs.float())\n",
    "        \n",
    "        info_dict = {'batch_idx':b_idx,'num_batches':len(trainloader2),'feats':feature_maps,'labels':labels}\n",
    "        pickle.dump(info_dict, handle)  \n",
    "        \n",
    "        # print GPU memory\n",
    "        print_memory_profile('After processing Batch')\n",
    "        \n",
    "        # free up GPU memory\n",
    "        del feature_maps, info_dict\n",
    "        torch.cuda.empty_cache()     \n",
    "        \n",
    "        # print GPU memory\n",
    "        print_memory_profile('After freeing GPU memory')\n",
    "        \n",
    "handle.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute PD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_pd.py\n",
    "\n",
    "def compute_pred_depth(arr):\n",
    "    last = arr[-1]\n",
    "\n",
    "    p_depth = 1\n",
    "    for i in range(len(arr)-1):\n",
    "        ele = arr[-1-(i+1)]\n",
    "        if ele!=last:\n",
    "            p_depth = (len(arr)-(i+1)) + 1\n",
    "            break\n",
    "    \n",
    "    return p_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = torch.randperm(len(testset))\n",
    "inds = perm[:num_test_imgs]\n",
    "\n",
    "labs = []\n",
    "samples = torch.empty((0,3,64,32))\n",
    "for i in tqdm(inds):\n",
    "    i = int(i)\n",
    "    labs.append(testset[i]['lab'])\n",
    "    samples = torch.cat((samples,testset[i]['img'].unsqueeze(0)))\n",
    "labs = torch.tensor(labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_resized = samples\n",
    "test_subset = torch.utils.data.TensorDataset(samples_resized,labs)\n",
    "testloader2 = torch.utils.data.DataLoader(test_subset, batch_size=128, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== Storing Batch Statistics =====================\n",
    "\n",
    "batch_info = {}\n",
    "train_embs_pkl_path = os.path.join(save_dir,f'{expt_name}.pkl')\n",
    "ckpt_path = os.path.join(save_dir,f'{expt_name}.pt')\n",
    "batch_info['readme'] = f'---- K={K} ---- ckpt_path={ckpt_path} ---- pkl_path={train_embs_pkl_path} ----'\n",
    "batch_info['imgs'] = [] # test images\n",
    "batch_info['preds'] = [] # corresponding model predictions\n",
    "batch_info['labels'] = [] # labels of the test images\n",
    "batch_info['pd'] = [] # corresponding prediction depths\n",
    "batch_info['layers_knn_prob'] = [] # for each test image we have a list of knn means for every layer\n",
    "batch_info['layers_knn_mode'] = [] # for each test image we have a list of knn mode for every layer\n",
    "\n",
    "print_memory_profile('Initial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over test images\n",
    "invalid_counter = 0 # for invalid predictions (last layer mode != model output)\n",
    "for test_id, (img,lab) in enumerate(tqdm(test_subset)):\n",
    "\n",
    "    batch_info['imgs'].append(img)\n",
    "    with torch.no_grad():\n",
    "        to_pil_trans = transforms.ToPILImage()\n",
    "        img = to_pil_trans(img.to('cuda'))\n",
    "        img = trans(img).unsqueeze(0)\n",
    "        lab = int(lab)\n",
    "        if img.shape[1]==4:\n",
    "            img = img[:,0,:,:].unsqueeze(0)\n",
    "        feature_maps = []\n",
    "        out = net(img)\n",
    "        print(f'Model output: {out}')\n",
    "        batch_info['preds'].append(int(out.argmax()))\n",
    "        batch_info['labels'].append(lab)\n",
    "\n",
    "        print_memory_profile('Model forward pass')\n",
    "        with open(train_embs_pkl_path, 'rb') as handle:            \n",
    "            info_dict = pickle.load(handle)\n",
    "            print_memory_profile('Pickle load')\n",
    "\n",
    "            # loop over layers in densenet, and compute KNN for this test image\n",
    "            knn_preds_mode = []  # layer-wise final KNN classification preds   \n",
    "            knn_preds_prob = []\n",
    "            for layer_id,feat in tqdm(enumerate(feature_maps)):\n",
    "                X_i = feat.unsqueeze(1)  # (10000, 1, 784) test set\n",
    "                X_j = info_dict['feats'][layer_id].unsqueeze(0)  # (1, 60000, 784) train set\n",
    "                if lp_norm==2:\n",
    "                    D_ij = ((X_i - X_j) ** 2).sum(-1)  # (10000, 60000) symbolic matrix of squared L2 distances\n",
    "                elif lp_norm==1:\n",
    "                    D_ij = (abs(X_i - X_j)).sum(-1)  # (10000, 60000) symbolic matrix of squared L2 distances\n",
    "                else:\n",
    "                    raise('Invalid lp_norm in arguments!')\n",
    "\n",
    "                ind_knn = torch.topk(-D_ij,K,dim=1)  # Samples <-> Dataset, (N_test, K)\n",
    "                lab_knn = info_dict['labels'][ind_knn[1]]  # (N_test, K) array of integers in [0,9]\n",
    "#                 print(f'!!!!!!test_img:{test_id}, nbrs in layer {layer_id}: {ind_knn[1]}')\n",
    "                mode = int(lab_knn.squeeze().mode()[0])\n",
    "                knn_preds_mode.append(mode)\n",
    "                knn_preds_prob.append(float((lab_knn==mode).float().mean()))\n",
    "\n",
    "            print_memory_profile('Pickle batch processed')\n",
    "\n",
    "            # free GPU memory\n",
    "            del info_dict\n",
    "            torch.cuda.empty_cache()\n",
    "            print_memory_profile('After GPU memory freed') \n",
    "\n",
    "            print('Test Image: %d' %(test_id))\n",
    "            print(f'knn_preds_mode: {knn_preds_mode}')\n",
    "            print(f'knn_preds_prob: {knn_preds_prob}')\n",
    "            print(f'label: {lab}')\n",
    "            print(f'pred: {int(out.argmax())}')\n",
    "            print('\\n')\n",
    "            batch_info['layers_knn_prob'].append(knn_preds_prob)\n",
    "            batch_info['layers_knn_mode'].append(knn_preds_mode)\n",
    "            if int(out.argmax())==knn_preds_mode[-1]: # PD accurate\n",
    "                if knn_pos_thresh==0.5 and knn_neg_thresh==0.5:\n",
    "                    batch_info['pd'].append(compute_pred_depth(knn_preds_mode))\n",
    "                else:\n",
    "                    raise('Code not ready yet! Compute pred arr function also has to be updated!')\n",
    "            else: # PD inaccurate, KNN pred doesn't match model pred\n",
    "                print('Invalid datapoint: last_layer_mode != model_output')\n",
    "                invalid_counter += 1\n",
    "                batch_info['pd'].append(-99)\n",
    "print(f'Invalid Counts Ratio: {invalid_counter}/{num_test_imgs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== Save results =====================\n",
    "\n",
    "with open(os.path.join(save_dir,expt_name+'_testPDinfo.pkl'), 'wb') as handle:\n",
    "    pickle.dump(batch_info, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot PDs histogram\n",
    "with open(os.path.join(save_dir,expt_name+'_testPDinfo.pkl'), 'rb') as handle:\n",
    "    batch_info = pickle.load(handle)\n",
    "\n",
    "batch_info['pd'] = np.array(batch_info['pd'])\n",
    "batch_info['labels'] = np.array(batch_info['labels'])\n",
    "batch_info['preds'] = np.array(batch_info['preds'])\n",
    "correct_preds_arr = (batch_info['preds']==batch_info['labels'])\n",
    "\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.title(expt_name)\n",
    "plt.ylabel('# of Images')\n",
    "plt.xlabel('Layer')\n",
    "if model=='resnet18':\n",
    "    plt.xlim((0,18))\n",
    "elif model=='vgg16':\n",
    "    plt.xlim((0,16))\n",
    "# plt.ylim((0,80))\n",
    "plt.hist(batch_info['pd'][correct_preds_arr],bins=10,color='g',alpha=0.55)\n",
    "# plt.hist(batch_info['pd'][~correct_preds_arr ],bins=100,color='r',alpha=0.55)\n",
    "plt.savefig(os.path.join(save_dir,expt_name+'_PDplot.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(batch_info['pd'][correct_preds_arr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model on Particular Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "total = 0\n",
    "for top in tqdm([0,1,2,4,5,6,7,8,9]):\n",
    "    for bot in [3]:\n",
    "        for i in range(20):\n",
    "            img_path = f'/xxx/home/xxx/xxxp/xxx/projects/shortcut_detection_and_mitigation/data/domino_datasets/top_cifar10_bot_fmnist/train/top{top}_bot{bot}/{int(random.random()*1000)}.jpg'\n",
    "            ckpt_path = '/xxx/home/xxx/xxxp/xxx/projects/shortcut_detection_and_mitigation/experiments/toy_expts/domino_expts/output/resnet18_top_cifar10_bot_fmnist_64x32_65168.pt'\n",
    "\n",
    "            net = ResNet18(num_channels=num_ch)\n",
    "            net = net.to(device)\n",
    "            net.linear = nn.Linear(in_features=1024,out_features=10,bias=True)\n",
    "            net = torch.nn.DataParallel(net)\n",
    "            net.load_state_dict(torch.load(ckpt_path)['net'])\n",
    "            net.eval()\n",
    "\n",
    "            T = torchvision.transforms.Compose([\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Resize((64,32)),\n",
    "            ])\n",
    "\n",
    "            with Image.open(img_path) as im:\n",
    "                out = net(T(im).unsqueeze(0))\n",
    "                total+=1\n",
    "                if int(out.argmax())==bot:\n",
    "                    counter+=1\n",
    "print(f'counter:{counter},total:{total},accuracy:{counter*100/total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'counter:{counter},total:{total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0:48/180, 26%,21%,80%,4.5,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Domino CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user hyperparams\n",
    "top_dset_name = 'cifar10'\n",
    "bot_dset_name = 'fmnist'\n",
    "top_inds = [0,1]\n",
    "bot_inds = [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['path','bottom_label','top_label','val_train_split','test_split','all_zeros'])\n",
    "\n",
    "# domino = 5000/800\n",
    "# blank = 6000/1000\n",
    "if top_dset_name=='blank':\n",
    "    arr = np.arange(0,6000,1)\n",
    "    np.random.shuffle(arr)\n",
    "    val_arr = arr[:600]\n",
    "    train_arr = arr[600:]\n",
    "    num_test_imgs = 1000\n",
    "else:\n",
    "    arr = np.arange(0,5000,1)\n",
    "    np.random.shuffle(arr)\n",
    "    val_arr = arr[:500]\n",
    "    train_arr = arr[500:]\n",
    "    num_test_imgs = 800\n",
    "\n",
    "for (top_idx,bot_idx) in zip(top_inds,bot_inds):\n",
    "    for img_id in tqdm(train_arr):\n",
    "        if top_dset_name=='blank':\n",
    "            path = f'/xxx/home/xxx/xxxp/xxx/projects/shortcut_detection_and_mitigation/data/domino_datasets/top_{top_dset_name}_bot_{bot_dset_name}/train/{bot_idx}/{img_id}.jpg'\n",
    "        else:    \n",
    "            path = f'/xxx/home/xxx/xxxp/xxx/projects/shortcut_detection_and_mitigation/data/domino_datasets/top_{top_dset_name}_bot_{bot_dset_name}/train/top{top_idx}_bot{bot_idx}/{img_id}.jpg'\n",
    "        df = df.append({'path':path, 'bot_lbl':bot_idx, 'top_lbl':top_idx, 'val_train_split':1, 'test_split':0, 'all_zeros':0}, ignore_index=True)\n",
    "        \n",
    "    for img_id in tqdm(val_arr):\n",
    "        if top_dset_name=='blank':\n",
    "            path = f'/xxx/home/xxx/xxxp/xxx/projects/shortcut_detection_and_mitigation/data/domino_datasets/top_{top_dset_name}_bot_{bot_dset_name}/train/{bot_idx}/{img_id}.jpg'\n",
    "        else:\n",
    "            path = f'/xxx/home/xxx/xxxp/xxx/projects/shortcut_detection_and_mitigation/data/domino_datasets/top_{top_dset_name}_bot_{bot_dset_name}/train/top{top_idx}_bot{bot_idx}/{img_id}.jpg'\n",
    "        df = df.append({'path':path, 'bot_lbl':bot_idx, 'top_lbl':top_idx, 'val_train_split':0, 'test_split':0, 'all_zeros':0}, ignore_index=True)\n",
    "        \n",
    "    for img_id in tqdm(range(num_test_imgs)):\n",
    "        if top_dset_name=='blank':            \n",
    "            path = f'/xxx/home/xxx/xxxp/xxx/projects/shortcut_detection_and_mitigation/data/domino_datasets/top_{top_dset_name}_bot_{bot_dset_name}/test/{bot_idx}/{img_id}.jpg'\n",
    "        else:\n",
    "            path = f'/xxx/home/xxx/xxxp/xxx/projects/shortcut_detection_and_mitigation/data/domino_datasets/top_{top_dset_name}_bot_{bot_dset_name}/test/top{top_idx}_bot{bot_idx}/{img_id}.jpg'\n",
    "        df = df.append({'path':path, 'bot_lbl':bot_idx, 'top_lbl':top_idx, 'val_train_split':2, 'test_split':1, 'all_zeros':0}, ignore_index=True)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_save_path = '/xxx/home/xxx/xxxp/xxx/projects/shortcut_detection_and_mitigation/experiments/toy_expts/domino_expts/top_cifar10_bot_fmnist_2class_ro1p0.csv'\n",
    "# df.to_csv(csv_save_path, index=False)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/xxx/home/xxx/xxxp/shared/Projects/shortcut_learning/WaterBirds_MetaData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = (df['bird_type']==1) & (df['land_type']==0)\n",
    "# arr = (df['test_split']==0) \n",
    "df[arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = (df['bird_type']==0) & (df['land_type']==0)\n",
    "df1 = df[arr1].sample(5620)\n",
    "\n",
    "arr2 = (df['bird_type']==0) & (df['land_type']==1)\n",
    "df2 = df[arr2].sample(300)\n",
    "\n",
    "arr3 = (df['bird_type']==1) & (df['land_type']==0)\n",
    "df3 = df[arr3].sample(80)\n",
    "\n",
    "arr4 = (df['bird_type']==1) & (df['land_type']==1)\n",
    "df4 = df[arr4].sample(1652)\n",
    "\n",
    "df_final = pd.concat([df1,df2,df3,df4]).sample(frac=1)\n",
    "df_final.to_csv('/xxx/home/xxx/xxxp/xxx/projects/shortcut_detection_and_mitigation/experiments/vision_expts/waterbirds/train.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = (df['bird_type']==0) & (df['land_type']==0)\n",
    "df1 = df[arr1].sample(600)\n",
    "\n",
    "arr2 = (df['bird_type']==0) & (df['land_type']==1)\n",
    "df2 = df[arr2].sample(2605)\n",
    "\n",
    "arr3 = (df['bird_type']==1) & (df['land_type']==0)\n",
    "df3 = df[arr3].sample(751)\n",
    "\n",
    "arr4 = (df['bird_type']==1) & (df['land_type']==1)\n",
    "df4 = df[arr4].sample(180)\n",
    "\n",
    "df_final = pd.concat([df1,df2,df3,df4]).sample(frac=1)\n",
    "df_final.to_csv('/xxx/home/xxx/xxxp/xxx/projects/shortcut_detection_and_mitigation/experiments/vision_expts/waterbirds/test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pl",
   "language": "python",
   "name": "pl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
